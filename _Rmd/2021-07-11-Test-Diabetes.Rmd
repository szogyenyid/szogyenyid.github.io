---
layout: post
title: "Dataset Analysis: Early stage diabetes risk prediction using XGBoost"
date: 2021-07-11 11:56:00 +0200
category: Data Science # Learning | Data Science | Security | Meta | Stories
tags:
    - r
    - xgboost
    - classification
    - medical
description: Predicting early stage diabetes using XGBoost (logistic gbtrees). The used dataset is from UCI Machine Learning Repository.
# last_modified_at: 2021-07-07 09:40:00 +0200
author: Daniel Szogyenyi
---

# Introducing the dataset

The data I used is from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Early+stage+diabetes+risk+prediction+dataset.#). The set has been collected using direct questionnaires from the patients of Sylhet Diabetes Hospital in Sylhet, Bangladesh and approved by a doctor.

# Including libraries

```{r}
library(readr) # for reading csv
library(xgboost) # for the prediction model
library(caret) # for data separation and confusion matrix
```

# Reading the file

```{r}
df <- read_csv("src/diabetes.csv", col_types = cols())
head(df)
```

# Preprocessing

## Cast all types to logical

Gender is mutated into "isMale", "Female" leading to FALSE, and "Male" to TRUE.

```{r}
df[df == "Yes" | df=="Positive" | df=="Male"] <- "T"
df[df == "No" | df=="Negative" | df=="Female"] <- "F"

df[2:17] <- apply(df[2:17],2,function(c){
  as.logical(unlist(c))
})

df$isMale <- df$Gender
df$Diabetes <- df$class
df$class <- NULL
df$Gender <- NULL
```

## Separate the dataset

Will use 80% of the data for training, and 20% for testing. "Diabetes" ratio in the subsets remain the same as in the whole set.

```{r}
idx <- caret::createDataPartition(df$Diabetes, p=0.8, list=F)
# Use every column but "diabetes" as data, and "diabetes" as label.
trainData <- xgb.DMatrix(data = as.matrix(df[idx,1:16]), label = unlist(df[idx,17]))
testData <- xgb.DMatrix(data = as.matrix(df[-idx,1:16]), label = unlist(df[-idx,17]))
```

# Creating the model

## Set up parameters

```{r}
params <- list(
  booster = "gbtree", # gradboosted trees
  objective = "binary:logistic", # as "having diabetes" is a binary attribute, use logistic
  eval_metric="logloss", # because the objective is logistic
  eta=0.5,
  gamma=0,
  max_depth=10,
  min_child_weight=1,
  subsample=1,
  colsample_bytree=1
)
```

## Cross-validate to get the ideal number of iterations

```{r}
xgbcv <- xgb.cv(
  params = params,
  data = trainData,
  nrounds = 500,
  nfold = 10,
  showsd = T,
  stratified = T,
  verbose=F
)
```

## Get the ideal number of iterations

```{r}
bestIter <- xgbcv$evaluation_log[xgbcv$evaluation_log$test_logloss_mean == min(xgbcv$evaluation_log$test_logloss_mean)]
print(bestIter)
```

## Create the model

```{r}
xgb1 <- xgb.train(params = params, data = trainData, nrounds = bestIter$iter, watchlist = list(test=testData,train=trainData), print_every_n = 10)
```

# Evaluation of the model

## Prediction on test dataset

Let's predict the test dataset.  
As logistic regression returns the probability of being true, if the value is larger than 0.5, let's say the person has diabetes.

```{r}
xgbpred <- predict(xgb1,testData)
xgbpred <- as.numeric(xgbpred>0.5)
```

## Analysing the confusion matrix

```{r}
confusionMatrix(as.factor(xgbpred), as.factor(as.numeric(unlist(df[-idx,17]))))
```
The model reached an accuracy of ~99%, with a high enough sensitivity and specifity.  
1 out of 40 people without diabetes was marked "having diabetes" and  
0 out of 64 people with diabetes was marked "not having diabetes".  

It's excellent that the latter number is 0 (so specificity is 100%), as it's better to mark healthy people as having diabetes, because further medical examinations can confirm or reject this prediction.

## Analysing the importance of features

```{r}
mat <- xgb.importance(feature_names = colnames(as.matrix(df[idx,1:16])), model = xgb1)
xgb.ggplot.importance(importance_matrix = mat, n_clusters = c(4,4))
```

The most important factor in the determination of someone having diabetes is polyuria. Not surprising that polydipsia is high too (because of the strong correlation betwen the two), as if someone consumes a lot of water, they will have a lot of urine.
The second more important factors are gender and age.  
Alopecia should be considered as an important factor, and everything in the "Cluster 1" (marked with red) is close to irrelevant.